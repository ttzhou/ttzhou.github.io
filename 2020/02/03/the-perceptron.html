<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <title>the perceptron.</title>
    <link rel="stylesheet" href="/assets/css/styles.css">

    <!-- Mathjax Support -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</head>


<body>
    <nav>
    
    <a href="/">
        About
    </a>
    
    <a href="/blog">
        Blog
    </a>
    
</nav>

    <h1>the perceptron.</h1>
<h5>Feb 03 2020</h5>

<div style="margin-top: 50px;">
<p>My job doesn’t really allow me to use machine learning or use interesting mathematics in a way that I would like to. As such, to keep both my knowledge fresh and give me something to do, I’ve decided to write what essentially amounts to expositions of important topics in statistical learning.</p>

<p>I find it most logical to start with one of the most basic machine learning algorithms: the perceptron. If you’re reading this, or any other of my “right” type posts (technical stuff) I generally assume basic understanding of linear algebra and probability, though I may elaborate on some of those details if it makes sense in the flow of things.</p>

<p>Anyhoo…</p>

<div style="margin-top: 50px;"></div>

<h3 id="the-perceptron">The Perceptron</h3>

<p>The perceptron is a classification algorithm that finds a separating hyperplane for data that can be linearly separated into two classes. That is, if the data can be represented as feature vectors \(\mathcal{T} = \{\vec{x}_i \}_{i = 1}^{N} \subseteq \mathbb{R}^n\), and \(y_i = f(x_i) \in \{-1, 1\}\) represents the class of \(\vec{x}_i\), then we are looking for \(\vec{w} \in \mathbb{R}^n\) such that</p>

\[y_i \vec{w}^{T} \vec{x}_i &gt; 0, \forall i\]

<p>Intuitively, we want \(\vec{w}\) so that the sign of \(\vec{w}^{T} \vec{x}_i\) (call this \(\hat{y}_i\)) always matches that of \(y_i\), the <em>actual</em> label of \(\vec{x}_i\), and that this works for all the given feature vectors \(\vec{x}_i\).</p>

<h4 id="the-algorithm"><strong>The Algorithm</strong></h4>

<p>Assume that the dataset is linearly separable; that is,</p>

\[\exists \vec{w}_*, ~~ \forall ~ \vec{x} \in \mathcal{T}, ~~ y_i \vec{w}_*^{T}\vec{x}_i &gt; 0 \qquad (1)\]

<p>The perceptron algorithm finds a \(\vec{w}\) such that \(\forall ~ \vec{x} \in \mathcal{T}, ~~ y_i \vec{w}^{T}\vec{x}_i &gt; 0\) in the following iterative manner:</p>

<ol>
  <li>
    <p>Initialize \(\vec{w}^{(1)} = \vec{0}\), and set iteration \(k\) to be 1.</p>
  </li>
  <li>At iteration \(k\), for each vector \(\vec{x}_i \in \mathcal{T}\), compute \(y_i {\vec{w}^{(k)}}^{T} \vec{x}_i\).
    <ul>
      <li>If all such \(y_i {\vec{w}^{(k)}}^{T} \vec{x}_i &gt; 0\), then \(\vec{w}^{(k)}\) is an appropriate weight vector that separates the classes, and the algorithm terminates.</li>
      <li>Otherwise, choose some \(\vec{x}^{(k)}\) where \(y^{(k)} {\vec{w}^{(k)}}^{T} \vec{x}^{(k)} \leq 0\), and update \(\vec{w}^{(k + 1)} = \vec{w}^{(k)} + y^{(k)} \vec{x}^{(k)}\).</li>
    </ul>

    <p>Increment iteration counter by 1 (set \(k = k + 1\)).</p>
  </li>
  <li>Repeat step 2 until there is a weight vector \(\vec{w}^{(K)}\) such that \(y_i {\vec{w}^{(K)}}^{T} \vec{x}_{i} &gt; 0\), for all \(\vec{x}_i \in \mathcal{T}\)</li>
</ol>

<p>Intuitively, what this procedure is doing is:</p>

<ul>
  <li>If I classify a vector correctly, I’m doing good - don’t do anything to the weight vector</li>
  <li>If I misclassify a vector, it means I need to pull the weight vector in the opposite direction it was going from the previous iteration. Then, I need to check everything all over again with the new weight vector.</li>
</ul>

<p>To prove that this algorithm works, it suffices to show that it terminates; more specifically, we show that \(K\), the number of iterations, is finite.</p>

<h4 id="proof-of-perceptron-algorithm"><strong>Proof of Perceptron Algorithm</strong></h4>

<p>The idea of the (standard) proof is to show that as \(\vec{w}^{(k)}\) is successively updated through the iterations, the bound for the angle between it and \(\vec{w}_*\) gets closer to 0, and the rate at which this happens increases with \(k\).</p>

<p><em>Proof.</em></p>

<p>Consider the weight vector after \(k\) iterations:</p>

\[\vec{w}^{(k)} = \vec{w}^{(k - 1)} + y^{(k - 1)} \vec{x}^{(k - 1)}\]

<p>Taking the dot product of both sides,</p>

\[\begin{align*}
    \left\lVert \vec{w}^{(k)} \right\rVert^2 &amp;= \left( \vec{w}^{(k - 1)} + y^{(k - 1)} \vec{x}^{(k - 1)} \right) \cdot \left( \vec{w}^{(k - 1)} + y^{(k - 1)} \vec{x}^{(k - 1)} \right) \cr
    &amp;= {\left\lVert \vec{w}^{(k - 1)} \right\rVert}^2 + 2 y^{(k - 1)} \vec{w}^{(k - 1)} \cdot \vec{x}^{(k - 1)} + \left\rVert y^{(k - 1)} \vec{x}^{(k - 1)} \right\rVert^2 \cr
    &amp;\leq {\left\lVert \vec{w}^{(k - 1)} \right\rVert}^2 + \left\rVert y^{(k - 1)} \vec{x}^{(k - 1)} \right\rVert^2
\end{align*}\]

<p>as \(2 y^{(k - 1)} \vec{w}^{(k - 1)} \cdot \vec{x}^{(k - 1)} \leq 0\) since \(\vec{w}^{(k - 1)}\) misclassified \(\vec{x}^{(k - 1)}\).</p>

<p>Noting that we can repeat this process downwards for \(k\), we get</p>

\[\begin{align*}
    \left\lVert \vec{w}^{(k)} \right\rVert^2 &amp;\leq {\left\lVert \vec{w}^{(k - 1)} \right\rVert}^2 + \left\rVert y^{(k - 1)} \vec{x}^{(k - 1)} \right\rVert^2 \cr
    &amp;\leq {\left\lVert \vec{w}^{(k - 2)} \right\rVert}^2 + \left\rVert y^{(k - 2)} \vec{x}^{(k - 2)} \right\rVert^2 + \left\rVert y^{(k - 1)} \vec{x}^{(k - 1)} \right\rVert^2 \cr
    &amp;\leq \vdots \cr
    &amp;\leq {\left\lVert \vec{w}^{(1)} \right\rVert}^2 + \left\rVert y^{(1)} \vec{x}^{(1)} \right\rVert^2 + \ldots + \left\rVert y^{(k - 1)} \vec{x}^{(k - 1)} \right\rVert^2 \cr
    &amp;\leq \left\rVert y^{(1)} \vec{x}^{(1)} \right\rVert^2 + \ldots + \left\rVert y^{(k - 1)} \vec{x}^{(k - 1)} \right\rVert^2 \cr
    &amp;\leq \left\rVert \vec{x}^{(1)} \right\rVert^2 + \ldots + \left\rVert \vec{x}^{(k - 1)} \right\rVert^2 \cr
\end{align*}\]

<p>since \(\vec{w}^{(1)} = \vec{0}\) and \(\left\lVert y^{(i)} \right\rVert = 1, \forall i\).</p>

<p>Let \(R = \displaystyle \max_{\mathcal{T}} \left\lVert \vec{x}_i \right\rVert\). Then</p>

\[\begin{align*}
    \left\lVert \vec{w}^{(k)} \right\rVert^2 &amp;\leq \left\rVert \vec{x}^{(1)} \right\rVert^2 + \ldots + \left\rVert \vec{x}^{(k - 1)} \right\rVert^2 \cr
    \left\lVert \vec{w}^{(k)} \right\rVert^2 &amp;\leq (k - 1)R^2 \cr
    \left\lVert \vec{w}^{(k)} \right\rVert &amp;\leq \sqrt{(k - 1)}R \qquad (2)
\end{align*}\]

<p>This gives an upper bound on the size of the weight vector at a given iteration.</p>

<p>Now, express the angle between \(\vec{w}_*\) and \(\vec{w}^{(k)}\) using their dot product.</p>

\[\begin{align*}
    \vec{w}^{T}_* \vec{w}^{(k)} &amp;= \vec{w}^{T} \vec{w}_*^{(k - 1)} + y^{(k - 1)} \vec{w}_*^{T}\vec{x}^{(k - 1)} \cr
    &amp;= \vec{w}^{T} \vec{w}_*^{(k - 2)} + y^{(k - 2)} \vec{w}_*^{T}\vec{x}^{(k - 2)} + y^{(k - 1)} \vec{w}_*^{T}\vec{x}^{(k - 1)} \cr
    &amp;= \vdots \cr
    &amp;= \vec{w}^{T} \vec{w}_*^{(1)} + y^{(1)} \vec{w}_*^{T}\vec{x}^{(1)} + \ldots + y^{(k - 1)} \vec{w}^{T}_* \vec{x}^{(k - 1)} \cr
    &amp;= y^{(1)} \vec{w}_*^{T}\vec{x}^{(1)} + \ldots + y^{(k - 1)} \vec{x}^{T}_* \vec{x}^{(k - 1)} \cr
\end{align*}\]

<p>Recall from \((1)\) that \(y_i \vec{w}^{T}_* x_i \gt 0, \forall i\). Then there must be some \(\delta &gt; 0\) such that \(y_i \vec{w}^{T}_* x_i \gt \delta, \forall i\), and hence</p>

\[\begin{align*}
    \vec{w}^{T}_* \vec{w}^{(k)} &amp;= y^{(1)} \vec{w}_*^{T}\vec{x}^{(1)} + \ldots + y^{(k - 1)} \vec{x}^{T}_* \vec{x}^{(k - 1)} \cr
    &amp;\geq (k - 1)\delta \cr

    \left\lVert \vec{w}^{\phantom{(}}_* \right\rVert \left\lVert \vec{w}^{(k)} \right\rVert \cos{\theta^{(k)}} &amp;\geq (k - 1)\delta \qquad (3)
\end{align*}\]

<p>where \(\theta^{(k)}\) is the angle between \(\vec{w}_*\) and \(\vec{w}^{(k)}\).</p>

<p>Without loss of generality, we can say that \(\vec{w}_*\) is a unit vector. Then, from \((2)\) and \((3)\),</p>

\[\cos{\theta^{(k)}} \geq \tfrac{(k - 1)\delta}{\left\lVert \vec{w}^{(k)} \right\rVert} \geq \tfrac{(k - 1)\delta}{\sqrt{(k - 1)}R} = \sqrt{k - 1}\tfrac{\delta}{R}\]

<p>Thus, \(\sqrt{k - 1} \tfrac{\delta}{R} \leq \cos{\theta^{(k)}} \leq 1\); rearranging for \(k\) gives \(k \leq \tfrac{R^2}{\delta^2} + 1\), which is a bound for the number of iterations the algorithm can take.</p>

<p>Therefore, the algorithm must terminate, and it can only terminate once the final weight vector correctly classifies all the points. \(\tag*{$\blacksquare$}\)</p>

<h4 id="limitations-of-the-perceptron"><strong>Limitations of the Perceptron</strong></h4>

<p>The first published paper on the perceptron does not coincide with the first person who thought of the idea. Nevertheless, the seminal paper was written by Frank Rosenblatt while working at the Cornell Aeronautical Laboratory <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. It came at a time when AI was a nascent field; as with any nascent field, initial discoveries and breakthroughs often contributed to an excessive optimism in what was possible (similar to where we are now in the AI landscape).</p>

<p>As we will see, however, the perceptron (as a proxy for this optimism) would be found to have some serious limitations. The key limitation can be found in the proof itself - the assumption that some \(\omega_{*}\) exists which can linearly separate the classes. There are of course cases where this is not true. To make this concrete, we illustrate the “canonical” counterexample.</p>

<p>Consider four points in \(\mathbb{R^2}\); \(\{ (x_1, x_2) \} = \{ (0, 0), (0, 1), (1, 0), (1, 1) \}\), and let</p>

\[f(\vec{x}) = \begin{cases} 1 &amp; \text{if } \min{(x, y)} = 0 \\ -1 &amp; \text{otherwise} \end{cases}\]

<p>Below, points with class \(1\) are colored blue, and those with class \(-1\) are colored red (these images were created using <a href="https://www.geogebra.org/?lang=en" style="text-decoration: none">Geogebra</a>).</p>

<p><img src="/assets/media/img/perceptron_2D_XOR.png" alt="test" height="450px" width="500px" /></p>

<p>We can see from this image that there is no way to draw a line that separates the two classes. Thus, the perceptron algorithm would fail here, but a human would easily be able to see the <em>rule</em> that separates the two classes. The problem here is that the rule is not a linear divider.</p>

<p>However, if we change our <em>perspective</em> on the problem, then a linear boundary becomes available. To see how this is done, note that the image shown appears to be two-dimensional.</p>

<p>If we instead view it as a plane in three dimensions \(\left( \mathbb{R}^3 \right)\), by adding a third coordinate, \(x_3\):</p>

<p><img src="/assets/media/img/perceptron_3D_XOR_1.png" alt="test" height="450px" width="500px" /></p>

<p>and rotate our view of the space like so:</p>

<p><img src="/assets/media/img/perceptron_3D_XOR_2.png" alt="test" height="450px" width="500px" /></p>

<p>we can see that a linear divider is indeed possible; one such divider is the plane defined by \(2x_3 - x_1 - x_2 + \tfrac{1}{2} = 0\):</p>

<p><img src="/assets/media/img/perceptron_3D_XOR_3.png" alt="test" height="450px" width="500px" /></p>

<p>The adding of a third coordinate to our samples is akin to adding a new feature to the feature vectors comprising our inputs.
 This process of adding features is known as <em>feature engineering</em>, and is an essential tool in improving the performance of classifiers (and machine learning models in general). Furthermore, setting machine learning aside completely, the general technique of adding information that can change our perspective on a problem, is a great principle to apply to solving any problem in general!</p>

<div style="margin-top:100px;"></div>

<h3 id="references">References</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><em>Perceptron</em> - Wikipedia. https://en.wikipedia.org/wiki/Perceptron <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</div>

    <footer>
    
    <a href="mailto:tim.zhou.55@gmail.com">
        
            <svg class="icon"><use xlink:href='#fas.fa-envelope'></use></svg>
        
        email
    </a>
    
    <a href="https://github.com/ttzhou">
        
            <svg class="icon"><use xlink:href='#fab.fa-github'></use></svg>
        
        github
    </a>
    
    <a href="https://www.linkedin.com/in/tim-zhou-0b2a1742/">
        
            <svg class="icon"><use xlink:href='#fab.fa-linkedin'></use></svg>
        
        linkedin
    </a>
    
</footer>

    <svg display="none" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
                  <defs>
                  <!--
                  Font Awesome Free 5.5.0 by @fontawesome - https://fontawesome.com
                  License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License)
                  -->
              <symbol id='fas.fa-envelope' viewBox='0 0 512 512'>
            <title>envelope</title>
            <path class='path1' d='M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z'></path>
        </symbol>
        <symbol id='fab.fa-github' viewBox='0 0 496 512'>
            <title>github</title>
            <path class='path1' d='M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z'></path>
        </symbol>
        <symbol id='fab.fa-linkedin' viewBox='0 0 448 512'>
            <title>linkedin</title>
            <path class='path1' d='M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z'></path>
        </symbol>
        </defs>
              </svg>

</body>
</html>
